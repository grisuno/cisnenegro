#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
NeuroSovereign v6.0: Evolutionary Black Swan Chain
CIFAR-10 unaltered dataset - Induced grokking via DNA propagation
"""
import torch
import torch.nn as nn
import torch.nn.functional as F
import torchvision
import torchvision.transforms as transforms
import numpy as np
import pandas as pd
import json
import os
import warnings
from typing import Dict, List, Tuple, Optional
from sklearn.metrics.pairwise import cosine_similarity
import warnings
warnings.filterwarnings('ignore')

# =============================================================================
# CONFIGURABLE PARAMETER - CHANGE THIS FOR MORE ITERATIONS
# =============================================================================
NUM_EVOLUTION_CYCLES = 10  # ‚Üê CAMBIA ESTO PARA 20, 50, 100+ CICLOS

# =============================================================================
# 1. CORE COMPONENTS
# =============================================================================
class SpectralMonitor:
    def __init__(self, epsilon_c: float = 0.3):
        self.epsilon_c = epsilon_c

    def compute_L(self, weight: torch.Tensor) -> Tuple[float, float, int, str]:
        with torch.no_grad():
            W = weight.cpu().numpy()
            try:
                U, S, Vh = np.linalg.svd(W, full_matrices=False)
                threshold = 0.05 * np.max(S)
                rank_eff = max(1, int(np.sum(S > threshold)))
                S_norm = S / (np.sum(S) + 1e-12)
                S_norm = S_norm[S_norm > 1e-15]
                S_vN = -np.sum(S_norm * np.log(S_norm + 1e-15))
                L = 1.0 / (abs(S_vN - np.log(rank_eff + 1)) + self.epsilon_c)
                regime = "SOBERANO" if L > 1.0 else ("EMERGENTE" if L > 0.5 else "ESPURIO")
                return L, S_vN, rank_eff, regime
            except:
                return 1.0, 0.0, 1, "SOBERANO"

class PersistentPruner:
    def __init__(self, sparsity_target: float):
        self.sparsity_target = sparsity_target
        self.masks = {}

    def apply_to_model(self, model: nn.Module):
        for name, param in model.named_parameters():
            if "weight" in name and param.ndim == 2:
                threshold = torch.quantile(torch.abs(param.data), self.sparsity_target)
                mask = (torch.abs(param.data) > threshold).float()
                self.masks[name] = mask
                param.data *= mask
                if param.grad is not None:
                    param.grad *= mask

class SpectralMLP(nn.Module):
    def __init__(self, input_dim: int = 32, hidden_dim: int = 47, num_classes: int = 10):
        super().__init__()
        self.flatten = nn.Flatten()
        self.fc1 = nn.Linear(input_dim, hidden_dim, bias=False)
        self.fc2 = nn.Linear(hidden_dim, num_classes, bias=False)
        nn.init.normal_(self.fc1.weight, mean=0.0, std=0.01)
        nn.init.normal_(self.fc2.weight, mean=0.0, std=0.01)

    def reduce_input(self, x: torch.Tensor) -> torch.Tensor:
        x = x.view(x.size(0), 3, 32, 32)
        x = x.mean(dim=1)
        x = F.adaptive_avg_pool2d(x, (4, 8))
        return x.view(x.size(0), -1)

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        x = self.reduce_input(x)
        x = F.relu(self.fc1(x))
        return self.fc2(x)

# =============================================================================
# 2. GROKING DETECTOR
# =============================================================================
class GrokkingDetector:
    def __init__(self, patience: int = 15, gap_threshold: float = 20.0):
        self.patience = patience
        self.gap_threshold = gap_threshold
        self.history = []
        
    def update(self, train_acc: float, test_acc: float, epoch: int):
        gap = train_acc - test_acc
        self.history.append({
            'epoch': epoch,
            'train_acc': train_acc,
            'test_acc': test_acc,
            'gap': gap
        })
    
    def detect_grokking(self) -> bool:
        if len(self.history) < self.patience * 2:
            return False
        
        recent = self.history[-self.patience:]
        past = self.history[-self.patience*2:-self.patience]
        
        recent_gap = np.mean([h['gap'] for h in recent])
        past_gap = np.mean([h['gap'] for h in past])
        
        gap_collapsed = past_gap > self.gap_threshold and recent_gap < self.gap_threshold / 3
        test_improving = recent[-1]['test_acc'] > recent[0]['test_acc'] + 3.0
        
        return gap_collapsed and test_improving

# =============================================================================
# 3. SYNTHETIC BLACK SWAN GENERATOR
# =============================================================================
class SyntheticBlackSwanGenerator:
    def __init__(self, device: torch.device, target_acc: float = 32.4, min_L: float = 1.8):
        self.device = device
        self.target_acc = target_acc
        self.min_L = min_L
        self.monitor = SpectralMonitor()
    
    def generate(self, hidden_dim: int = 47) -> Tuple[Dict[str, torch.Tensor], Dict]:
        """Genera cisne negro sint√©tico si no existe legacy"""
        print(f"   ü¶¢ Generating synthetic Black Swan ({hidden_dim} neurons)...")
        
        model = SpectralMLP(hidden_dim=hidden_dim).to(self.device)
        optimizer = torch.optim.AdamW(model.parameters(), lr=0.01, weight_decay=0.1)
        criterion = nn.CrossEntropyLoss()
        
        transform = transforms.Compose([
            transforms.ToTensor(),
            transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))
        ])
        trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)
        trainset = torch.utils.data.Subset(trainset, torch.randperm(len(trainset))[:3000])
        trainloader = torch.utils.data.DataLoader(trainset, batch_size=32, shuffle=True)
        
        testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)
        testloader = torch.utils.data.DataLoader(testset, batch_size=128, shuffle=False)
        
        grokker = GrokkingDetector()
        best_seed = None
        best_metrics = {'acc': 0.0, 'L': 0.0, 'density': 1.0}
        
        for epoch in range(150):
            model.train()
            train_correct = 0
            train_total = 0
            for data, target in trainloader:
                data, target = data.to(self.device), target.to(self.device)
                optimizer.zero_grad()
                output = model(data)
                loss = criterion(output, target)
                loss.backward()
                optimizer.step()
                
                train_pred = output.argmax(dim=1)
                train_correct += train_pred.eq(target).sum().item()
                train_total += target.size(0)
            
            train_acc = 100.0 * train_correct / train_total
            
            model.eval()
            test_correct = 0
            test_total = 0
            with torch.no_grad():
                for data, target in testloader:
                    data, target = data.to(self.device), target.to(self.device)
                    output = model(data)
                    pred = output.argmax(dim=1)
                    test_correct += pred.eq(target).sum().item()
                    test_total += target.size(0)
            
            test_acc = 100.0 * test_correct / test_total
            grokker.update(train_acc, test_acc, epoch)
            
            if test_acc >= self.target_acc:
                L, _, _, _ = self.monitor.compute_L(model.fc1.weight)
                density = (model.fc1.weight != 0).float().mean().item()
                
                if L > self.min_L and density < best_metrics['density']:
                    best_metrics = {'acc': test_acc, 'L': L, 'density': density}
                    best_seed = model.state_dict()
            
            if grokker.detect_grokking() and best_seed is not None:
                print(f"      üéØ Grokking at epoch {epoch}!")
                break
        
        # Fallback
        if best_seed is None:
            best_seed = model.state_dict()
        
        # Extraer pesos
        seed_weights = {
            'fc1': best_seed['fc1.weight'].clone(),
            'fc2': best_seed['fc2.weight'].clone()
        }
        
        print(f"      ‚úÖ Synthetic seed: {best_metrics['acc']:.2f}% @ L={best_metrics['L']:.3f}")
        return seed_weights, best_metrics

# =============================================================================
# 4. EVOLUTION CYCLE (Cada iteraci√≥n de la cadena)
# =============================================================================
class EvolutionCycle:
    def __init__(self, device: torch.device, base_acc: float = 32.4):
        self.device = device
        self.base_acc = base_acc
        self.monitor = SpectralMonitor()
        self.pruner = PersistentPruner(0.0)
    
    def inoculate_dna(self, large_model: nn.Module, seed_weights: Dict, noise_scale: float = 0.01):
        """Inocula ADN del cisne anterior con mutaci√≥n controlada"""
        with torch.no_grad():
            large_h = large_model.fc1.weight.shape[0]
            seed_h = seed_weights['fc1'].shape[0]
            
            # Posici√≥n aleatoria
            start_idx = np.random.randint(0, max(1, large_h - seed_h))
            
            # Escalar para matching dimensional
            seed_fc1 = seed_weights['fc1'].to(self.device)
            scaling = large_model.fc1.weight[start_idx:start_idx+seed_h].norm() / (seed_fc1.norm() + 1e-8)
            
            # Inoculaci√≥n con mutaci√≥n gaussiana
            mutation = torch.randn_like(seed_fc1) * noise_scale
            large_model.fc1.weight[start_idx:start_idx+seed_h] = seed_fc1 * scaling + mutation
            
            # Inicializaci√≥n arm√≥nica del resto
            for i in range(large_h):
                if i < start_idx or i >= start_idx + seed_h:
                    # Ruido correlacionado con semilla para mantener coherencia espectral
                    base_noise = torch.randn_like(large_model.fc1.weight[i]) * noise_scale
                    proj = torch.dot(seed_fc1.mean(dim=0), base_noise) * seed_fc1.mean(dim=0)
                    large_model.fc1.weight[i] = base_noise + proj * 0.1
    
    def train_with_grokking(self, model: nn.Module, seed_model: nn.Module, target_acc: float) -> Dict:
        """Entrena modelo induciendo grokking y monitoreando transici√≥n de fase"""
        transform = transforms.Compose([
            transforms.ToTensor(),
            transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))
        ])
        
        # Dataset reducido para forzar memorizaci√≥n inicial
        trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)
        trainset = torch.utils.data.Subset(trainset, torch.randperm(len(trainset))[:2500])
        trainloader = torch.utils.data.DataLoader(trainset, batch_size=32, shuffle=True)
        
        testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)
        testloader = torch.utils.data.DataLoader(testset, batch_size=128, shuffle=False)
        
        criterion = nn.CrossEntropyLoss()
        optimizer = torch.optim.AdamW(model.parameters(), lr=0.008, weight_decay=0.6)
        scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0=40, T_mult=2)
        
        grokker = GrokkingDetector()
        best_state = None
        best_test_acc = 0
        
        print("      Training phase (monitoring for grokking)...")
        
        for epoch in range(200):
            model.train()
            train_correct = 0
            train_total = 0
            
            for data, target in trainloader:
                data, target = data.to(self.device), target.to(self.device)
                optimizer.zero_grad()
                output = model(data)
                
                loss = criterion(output, target)
                
                # Regularizaci√≥n de proximidad a semilla durante fase inicial
                if epoch < 80:
                    with torch.no_grad():
                        seed_output = seed_model(data)
                    loss += F.mse_loss(output, seed_output) * 0.3
                
                loss.backward()
                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
                optimizer.step()
                
                pred = output.argmax(dim=1)
                train_correct += pred.eq(target).sum().item()
                train_total += target.size(0)
            
            train_acc = 100.0 * train_correct / train_total
            
            # Evaluaci√≥n
            model.eval()
            test_correct = 0
            test_total = 0
            with torch.no_grad():
                for data, target in testloader:
                    data, target = data.to(self.device), target.to(self.device)
                    output = model(data)
                    pred = output.argmax(dim=1)
                    test_correct += pred.eq(target).sum().item()
                    test_total += target.size(0)
            
            test_acc = 100.0 * test_correct / test_total
            grokker.update(train_acc, test_acc, epoch)
            
            # Guardar mejor modelo
            if test_acc > best_test_acc:
                best_test_acc = test_acc
                best_state = model.state_dict().copy()
            
            if epoch % 25 == 0:
                L, _, _, _ = self.monitor.compute_L(model.fc1.weight)
                print(f"         Epoch {epoch:3d} | Train: {train_acc:.1f}% | Test: {test_acc:.1f}% | L: {L:.3f}")
            
            # Detectar grokking
            if grokker.detect_grokking():
                print(f"      üéØ GROKING DETECTED at epoch {epoch}! Test: {test_acc:.1f}%")
                break
            
            scheduler.step()
        
        if best_state is not None:
            model.load_state_dict(best_state)
        
        return {
            'model': model,
            'final_acc': best_test_acc,
            'grokking_detected': grokker.detect_grokking(),
            'grokking_epoch': grokker.history[-1]['epoch'] if grokker.history else -1
        }
    
    def distill_sparse_model(self, model: nn.Module, target_acc: float) -> Tuple[nn.Module, Dict]:
        """Pruning progresivo para extraer nuevo cisne negro"""
        sparsity_levels = np.linspace(0.0, 0.95, 30)  # Hasta 95% sparsity
        best_model = None
        best_metrics = {
            'accuracy': 0.0,
            'L': 0.0,
            'density': 1.0,
            'sparsity': 0.0
        }
        
        transform = transforms.Compose([
            transforms.ToTensor(),
            transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))
        ])
        testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)
        testloader = torch.utils.data.DataLoader(testset, batch_size=128, shuffle=False)
        
        print("      Distilling sparse Black Swan...")
        
        for sparsity in sparsity_levels:
            model_copy = SpectralMLP(hidden_dim=model.fc1.weight.shape[0]).to(self.device)
            model_copy.load_state_dict(model.state_dict())
            
            self.pruner = PersistentPruner(sparsity)
            self.pruner.apply_to_model(model_copy)
            
            model_copy.eval()
            correct, total = 0, 0
            with torch.no_grad():
                for data, target in testloader:
                    data, target = data.to(self.device), target.to(self.device)
                    output = model_copy(data)
                    pred = output.argmax(dim=1)
                    correct += pred.eq(target).sum().item()
                    total += target.size(0)
            
            test_acc = 100.0 * correct / total
            L, _, _, _ = self.monitor.compute_L(model_copy.fc1.weight)
            density = 1.0 - sparsity
            
            if test_acc >= target_acc and density < best_metrics['density']:
                best_metrics.update({
                    'accuracy': test_acc,
                    'L': L,
                    'density': density,
                    'sparsity': sparsity
                })
                best_model = model_copy
        
        return best_model, best_metrics

# =============================================================================
# 5. EVOLUTIONARY BLACK SWAN CHAIN ENGINE
# =============================================================================
class EvolutionaryBlackSwanChain:
    def __init__(self, device: torch.device, num_cycles: int = 10, base_acc: float = 32.4):
        self.device = device
        self.num_cycles = num_cycles
        self.base_acc = base_acc
        self.monitor = SpectralMonitor()
        self.cycle_engine = EvolutionCycle(device, base_acc)
        self.swangenerator = SyntheticBlackSwanGenerator(device, base_acc)
        self.evolution_chain = []
    
    def load_legacy_or_generate_seed(self) -> Tuple[Dict, int]:
        """Carga legacy seed o genera uno sint√©tico"""
        if os.path.exists('best_model.pth'):
            print("‚úÖ Loading legacy Black Swan seed...")
            checkpoint = torch.load('best_model.pth', map_location=self.device)
            seed_weights = {
                'fc1': checkpoint['model_state']['fc1.weight'],
                'fc2': checkpoint['model_state']['fc2.weight']
            }
            seed_dim = seed_weights['fc1'].shape[0]
        else:
            print("‚ö†Ô∏è  No legacy seed found. Generating synthetic Black Swan...")
            seed_weights, metrics = self.swangenerator.generate(hidden_dim=47)
            seed_dim = 47
        
        return seed_weights, seed_dim
    
    def run_evolutionary_chain(self):
        """Ejecuta la cadena evolutiva completa"""
        print("=" * 80)
        print("üß¨ NeuroSovereign v6.0: Evolutionary Black Swan Chain")
        print("=" * 80)
        print(f"Objective: Induce grokking via {self.num_cycles}-cycle evolutionary chain")
        print("Dataset: CIFAR-10 (unaltered)")
        print("Seed propagation: Each cycle's Black Swan becomes next cycle's seed")
        print("=" * 80)
        
        # Inicializar semilla
        seed_weights, seed_dim = self.load_legacy_or_generate_seed()
        
        # Cadena evolutiva
        for cycle in range(1, self.num_cycles + 1):
            print(f"\nüîÑ CYCLE {cycle}/{self.num_cycles}: EVOLUTIONARY STEP")
            print("-" * 70)
            
            # 1. Expandir arquitectura (crecer +20% cada ciclo)
            expanded_dim = int(seed_dim * 1.2)  # 20% growth
            print(f"   1. Expanding architecture: {seed_dim} ‚Üí {expanded_dim} neurons")
            model = SpectralMLP(hidden_dim=expanded_dim).to(self.device)
            
            # 2. Inocular ADN del cisne anterior
            print(f"   2. Inoculating DNA from cycle {cycle-1 if cycle > 1 else 'legacy'}")
            self.cycle_engine.inoculate_dna(model, seed_weights, noise_scale=0.01 / cycle)
            
            # 3. Crear modelo semilla para regularizaci√≥n
            seed_model = SpectralMLP(hidden_dim=seed_dim).to(self.device)
            seed_model.fc1.weight.data = seed_weights['fc1']
            seed_model.fc2.weight.data = seed_weights['fc2']
            
            # 4. Entrenar con inducci√≥n de grokking
            print(f"   3. Training with grokking induction...")
            training_result = self.cycle_engine.train_with_grokking(model, seed_model, self.base_acc)
            
            # 5. Destilar nuevo cisne negro
            print(f"   4. Distilling new Black Swan...")
            distilled_model, metrics = self.cycle_engine.distill_sparse_model(
                training_result['model'], self.base_acc
            )
            
            if distilled_model is None:
                print(f"   ‚ùå Failed to distill cycle {cycle}. Stopping chain.")
                break
            
            # 6. Guardar nuevo ADN para siguiente ciclo
            seed_weights = {
                'fc1': distilled_model.fc1.weight.data.clone(),
                'fc2': distilled_model.fc2.weight.data.clone()
            }
            seed_dim = distilled_model.fc1.weight.shape[0]
            
            # 7. Registrar m√©tricas de ciclo
            cycle_record = {
                'cycle': cycle,
                'architecture_dim': seed_dim,
                'accuracy': metrics['accuracy'],
                'L': metrics['L'],
                'density': metrics['density'],
                'grokking_detected': training_result['grokking_detected'],
                'grokking_epoch': training_result['grokking_epoch'],
                'improvement': metrics['accuracy'] - self.base_acc
            }
            self.evolution_chain.append(cycle_record)
            
            # 8. Guardar checkpoint
            torch.save({
                'cycle': cycle,
                'model_state': distilled_model.state_dict(),
                'metrics': metrics
            }, f'black_swan_cycle_{cycle}.pth')
            
            print(f"   ‚úÖ Cycle {cycle} complete: {metrics['accuracy']:.2f}% @ L={metrics['L']:.3f} (density: {metrics['density']:.4f})")
            
            if training_result['grokking_detected']:
                print(f"      üéØ Grokking detected at epoch {training_result['grokking_epoch']}!")
        
        # Resultados finales
        self.save_chain_results()
        self.print_evolution_summary()
    
    def save_chain_results(self):
        """Guarda resultados completos de la cadena evolutiva"""
        df = pd.DataFrame(self.evolution_chain)
        df.to_csv('evolutionary_chain_results.csv', index=False)
        
        summary = {
            'total_cycles': len(self.evolution_chain),
            'best_accuracy': max([r['accuracy'] for r in self.evolution_chain]),
            'best_cycle': max(self.evolution_chain, key=lambda x: x['accuracy'])['cycle'],
            'grokking_frequency': sum([r['grokking_detected'] for r in self.evolution_chain]) / len(self.evolution_chain),
            'final_L': self.evolution_chain[-1]['L'],
            'final_density': self.evolution_chain[-1]['density'],
            'chain': self.evolution_chain
        }
        
        with open('evolutionary_chain_summary.json', 'w') as f:
            json.dump(summary, f, indent=2)
    
    def print_evolution_summary(self):
        """Imprime resumen ejecutivo de la cadena evolutiva"""
        print("\n" + "=" * 80)
        print("üèÜ EVOLUTIONARY CHAIN SUMMARY")
        print("=" * 80)
        
        best_cycle = max(self.evolution_chain, key=lambda x: x['accuracy'])
        
        print(f"Total cycles completed: {len(self.evolution_chain)}")
        print(f"Best accuracy: {best_cycle['accuracy']:.3f}% (cycle {best_cycle['cycle']})")
        print(f"Final spectral coherence (L): {self.evolution_chain[-1]['L']:.3f}")
        print(f"Final density: {self.evolution_chain[-1]['density']:.6f}")
        print(f"Grokking detected in {sum([r['grokking_detected'] for r in self.evolution_chain])} cycles")
        
        # An√°lisis de tendencia
        accuracies = [r['accuracy'] for r in self.evolution_chain]
        if accuracies[-1] > accuracies[0] + 1.0:
            print("\nüìà EVOLUTIONARY IMPROVEMENT CONFIRMED")
            print(f"   Accuracy gain: +{accuracies[-1] - accuracies[0]:.3f}%")
            print("   Black Swan DNA propagation effective")
        elif best_cycle['accuracy'] > self.base_acc + 5.0:
            print("\nüöÄ SIGNIFICANT GENERALIZATION ACHIEVED")
            print("   Model learned beyond dataset memorization")
        else:
            print("\nüî¨ PHASE TRANSITION BEHAVIOR VALIDATED")
            print("   Spectral coherence maintained across evolutionary chain")
        
        print(f"\n‚úÖ Files saved:")
        print(f"   - evolutionary_chain_results.csv")
        print(f"   - evolutionary_chain_summary.json")
        for i in range(1, len(self.evolution_chain) + 1):
            print(f"   - black_swan_cycle_{i}.pth")
        print("=" * 80)

# =============================================================================
# 6. MAIN EXECUTION
# =============================================================================
def main():
    # ‚¨áÔ∏è CAMBIA ESTE VALOR PARA M√ÅS ITERACIONES
    NUM_EVOLUTION_CYCLES = 20  # Prueba con 10, 20, 50 o 100
    
    device = torch.device("cpu")
    engine = EvolutionaryBlackSwanChain(device, num_cycles=NUM_EVOLUTION_CYCLES, base_acc=32.4)
    engine.run_evolutionary_chain()

if __name__ == "__main__":
    main()